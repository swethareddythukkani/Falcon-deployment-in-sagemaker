{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI/WuaE+Eh7VrGmAQ0bLUS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/Falcon-40B-deployment-in-sagemaker/blob/main/Falcon_40B_deployment_in_Sagemaker_%2B_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we'll host a llm on Amazon SageMaker using Hugging Face LLM Inference Container for Amazon SageMaker, which allows you to easily deploy the most popular open-source LLMs, including Falcon, StarCoder, BLOOM, GPT-NeoX, Llama, and T5."
      ],
      "metadata": {
        "id": "-J7dYAn41DxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------"
      ],
      "metadata": {
        "id": "ZnHxgmw51Vam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Background and Details"
      ],
      "metadata": {
        "id": "1Bl3o1MM1TdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be working with Falcon-40B-Instruct that was developed by the Technology Innovation Institute (TII). Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license."
      ],
      "metadata": {
        "id": "ZZA-Js7u1Npq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------"
      ],
      "metadata": {
        "id": "Ez44b-D31Wr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install dependencies"
      ],
      "metadata": {
        "id": "TxA2iNvA1liF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade boto3 sagemaker #upgrades boto3 and sagemaker libraires"
      ],
      "metadata": {
        "id": "uCqT9P8_1eBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Create a bucket"
      ],
      "metadata": {
        "id": "eKfmmxpy1e8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "sess = sagemaker.Session()\n",
        "\n",
        "sagemaker_session_bucket=None #  sagemaker session bucket -> used for uploading data, models and logs\n",
        "                              # sagemaker will automatically create this bucket if it not exists\n",
        "\n",
        "\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    sagemaker_session_bucket = sess.default_bucket() # set to default bucket if a bucket name is not given"
      ],
      "metadata": {
        "id": "pGsO-1z21qto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Role, so we can use it to call models and buckets"
      ],
      "metadata": {
        "id": "73U9kJbX1sf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    role = sagemaker.get_execution_role() # try calling role directly\n",
        "\n",
        "except ValueError:\n",
        "    iam = boto3.client('iam')\n",
        "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn'] # it direct call fails use this code\n",
        "\n",
        "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
        "\n",
        "print(f\"sagemaker role arn: {role}\") # we use roles to call models\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")"
      ],
      "metadata": {
        "id": "I7u3GkIj1xJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get image of hugging_face_llm to acutally get models from hf-hub and run them as containers locally"
      ],
      "metadata": {
        "id": "FUcjuIDQ18yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface import get_huggingface_llm_image_uri # HuggingFaceModel model class with a image_uri pointing to the image.\n",
        "                                                                # To retrieve the new Hugging Face LLM Deep Learning Container in Amazon SageMaker,\n",
        "\n",
        "# retrieve the llm image uri\n",
        "llm_image = get_huggingface_llm_image_uri(\n",
        "  \"huggingface\",\n",
        "  version=\"0.8.2\"\n",
        ")\n",
        "\n",
        "# print ecr image uri\n",
        "print(f\"llm image uri: {llm_image}\")"
      ],
      "metadata": {
        "id": "o5Pq_kpy2TRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deploy Falcon-40B-Instruct model to Amazon SageMaker, we create a HuggingFaceModel model class and define our endpoint configuration including the hf_model_id, and instance_type. We will use a g5.12xlarge instance type with 4 NVIDIA A10G GPUs and 96GB of GPU memory."
      ],
      "metadata": {
        "id": "dcjGcc9_3EEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Model from importing it from HF-Hub"
      ],
      "metadata": {
        "id": "QK4H6-593Ov6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "# sagemaker config\n",
        "instance_type = \"ml.g5.12xlarge\"\n",
        "number_of_gpu = 4\n",
        "\n",
        "# TGI config\n",
        "config = {\n",
        "  'HF_MODEL_ID': \"tiiuae/falcon-40b-instruct\", # model id from hf.co/models\n",
        "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
        "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
        "  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
        "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
        "}\n",
        "\n",
        "# create HuggingFaceModel\n",
        "llm_model = HuggingFaceModel(\n",
        "  role=role,\n",
        "  image_uri=llm_image, # optional parameter to use when deploying as container\n",
        "  env=config\n",
        ")"
      ],
      "metadata": {
        "id": "fRsIxOs39Ju_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy Model as EndPoint"
      ],
      "metadata": {
        "id": "gIuyFgLG9Lju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define payload - simply a command to tell what prompt to execute in llm and how to execute it\n",
        "prompt = \"\"\"You are an helpful Assistant, called Falcon. Knowing everyting about AWS.\n",
        "User: Can you tell me something about Amazon SageMaker?\n",
        "Falcon:\"\"\"\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "P1ulBD6U9VAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt - Engineering"
      ],
      "metadata": {
        "id": "hBTIhFeaBIli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt engineering is a technique used to design effective prompts for LLMs with the goal to achieve: Control over the output, Mitigate Bias, Improve Model Efficiency"
      ],
      "metadata": {
        "id": "wwddYpWwBujf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt engineered Prompt template: 1) Instruction - a specific task or instruction you want the model to perform\n",
        "#                                    2) Context - can involve external information or additional context that can steer the model to better responses\n",
        "#                                    3) Input Data - is the input or question that we are interested to find a response for\n",
        "#                                    4) Output Indicator - indicates the type or format of output."
      ],
      "metadata": {
        "id": "QqCc2P6hCFJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple unstructured prompt\n",
        "prompt = \"\"\"\n",
        "Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "User: What was OKT3 originally sourced from?\n",
        "\n",
        "Falcon:\"\"\"\n",
        "\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "9sOa1iMsCaoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Engineered prompt with above 2nd cell template\n",
        "prompt = \"\"\"\n",
        "Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "8Pq9Cd5aCmMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEW-SHOT LEARNING"
      ],
      "metadata": {
        "id": "w3d2jcjECwV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot learning in prompt engineering involves providing a model with a few examples (typically 2-5) of a task within the prompt to guide its understanding and response. This helps the model generalize and perform the task with minimal training data."
      ],
      "metadata": {
        "id": "bUFebrNoC3lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-shot - means no examples or references are provides relying only on model interpretation to get output\n",
        "\n",
        "prompt = \"\"\"\n",
        "Tweet: \"This new music video was incredibile\"\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "Xc_4ngn4DB_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With Few-shot technique\n",
        "prompt = \"\"\"\n",
        "Tweet: \"I hate it when my phone battery dies.\"\n",
        "Sentiment: Negative\n",
        "###\n",
        "Tweet: \"My day has been üëç\"\n",
        "Sentiment: Positive\n",
        "###\n",
        "Tweet: \"This is the link to the article\"\n",
        "Sentiment: Neutral\n",
        "###\n",
        "Tweet: \"This new music video was incredibile\"\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "jCHS2yTJDfdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear up resources"
      ],
      "metadata": {
        "id": "PyWXJgDiBf9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.delete_model()\n",
        "llm.delete_endpoint()"
      ],
      "metadata": {
        "id": "IDmJwl3i-J5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "also manually check and delete s3 and stop sagemaker space and after stoping delete it"
      ],
      "metadata": {
        "id": "MUVKm4gXBi1l"
      }
    }
  ]
}